{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e85a5e-8bd4-47dd-9605-0ccc403058ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8dba14d-fbc4-4a6b-8d9e-1c041ede1a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caeecaa4-3db7-4383-b089-e5d4fb8e60fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Natural language processing (NLP) is a field of artificial intelligence that enables computers to understand, interpret, and generate human language. \n",
    "NLP is used in various applications, including chatbots, language translation, and sentiment analysis.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfe88af7-5de8-4dbc-b34b-d60e188211a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d148fe1-8d75-4944-b03f-79c822e5013b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'enables', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'NLP', 'is', 'used', 'in', 'various', 'applications', ',', 'including', 'chatbots', ',', 'language', 'translation', ',', 'and', 'sentiment', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Tokenization\n",
    "tokens = word_tokenize(sample_text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9062642d-7f98-4a96-93a1-19d74a7db4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS Tags: [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('field', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('that', 'WDT'), ('enables', 'VBZ'), ('computers', 'NNS'), ('to', 'TO'), ('understand', 'VB'), (',', ','), ('interpret', 'VB'), (',', ','), ('and', 'CC'), ('generate', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.'), ('NLP', 'NNP'), ('is', 'VBZ'), ('used', 'VBN'), ('in', 'IN'), ('various', 'JJ'), ('applications', 'NNS'), (',', ','), ('including', 'VBG'), ('chatbots', 'NNS'), (',', ','), ('language', 'NN'), ('translation', 'NN'), (',', ','), ('and', 'CC'), ('sentiment', 'NN'), ('analysis', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: POS Tagging (Part of Speech)\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"\\nPOS Tags:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bfc71dc-a921-4013-945d-27b05408204d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered Tokens (No stop words): ['Natural', 'language', 'processing', '(', 'NLP', ')', 'field', 'artificial', 'intelligence', 'enables', 'computers', 'understand', ',', 'interpret', ',', 'generate', 'human', 'language', '.', 'NLP', 'used', 'various', 'applications', ',', 'including', 'chatbots', ',', 'language', 'translation', ',', 'sentiment', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Stop words removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"\\nFiltered Tokens (No stop words):\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae88a94d-3364-4e59-86ec-099f95776279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemmed Tokens: ['natur', 'languag', 'process', '(', 'nlp', ')', 'field', 'artifici', 'intellig', 'enabl', 'comput', 'understand', ',', 'interpret', ',', 'gener', 'human', 'languag', '.', 'nlp', 'use', 'variou', 'applic', ',', 'includ', 'chatbot', ',', 'languag', 'translat', ',', 'sentiment', 'analysi', '.']\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"\\nStemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33779c6b-ec97-42f3-9329-f1957d32cc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized Tokens: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'field', 'artificial', 'intelligence', 'enables', 'computer', 'understand', ',', 'interpret', ',', 'generate', 'human', 'language', '.', 'NLP', 'used', 'various', 'application', ',', 'including', 'chatbots', ',', 'language', 'translation', ',', 'sentiment', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"\\nLemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f240b71d-038d-45fb-bd9e-e5b73aa5b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: TF-IDF Calculation\n",
    "# Here, we are considering our sample document as a collection of one document for the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([sample_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93ddb172-8e4d-483a-8746-190ba7605f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Matrix:\n",
      "   analysis       and  applications  artificial  chatbots  computers  \\\n",
      "0  0.150756  0.301511      0.150756    0.150756  0.150756   0.150756   \n",
      "\n",
      "    enables     field  generate     human  ...       nlp        of  \\\n",
      "0  0.150756  0.150756  0.150756  0.150756  ...  0.301511  0.150756   \n",
      "\n",
      "   processing  sentiment      that        to  translation  understand  \\\n",
      "0    0.150756   0.150756  0.150756  0.150756     0.150756    0.150756   \n",
      "\n",
      "       used   various  \n",
      "0  0.150756  0.150756  \n",
      "\n",
      "[1 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get the TF-IDF values as a dataframe\n",
    "import pandas as pd\n",
    "df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7f0c434-5a18-415e-9d59-cb2ee7490647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Term Frequency (TF):\n",
      "analysis        0.150756\n",
      "and             0.301511\n",
      "applications    0.150756\n",
      "artificial      0.150756\n",
      "chatbots        0.150756\n",
      "computers       0.150756\n",
      "enables         0.150756\n",
      "field           0.150756\n",
      "generate        0.150756\n",
      "human           0.150756\n",
      "in              0.150756\n",
      "including       0.150756\n",
      "intelligence    0.150756\n",
      "interpret       0.150756\n",
      "is              0.301511\n",
      "language        0.452267\n",
      "natural         0.150756\n",
      "nlp             0.301511\n",
      "of              0.150756\n",
      "processing      0.150756\n",
      "sentiment       0.150756\n",
      "that            0.150756\n",
      "to              0.150756\n",
      "translation     0.150756\n",
      "understand      0.150756\n",
      "used            0.150756\n",
      "various         0.150756\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Term Frequency (TF)\n",
    "print(\"\\nTerm Frequency (TF):\")\n",
    "tf_values = df_tfidf.sum(axis=0)\n",
    "print(tf_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b547ed1-1dd4-4ce6-a557-70ab2591a777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "              IDF\n",
      "analysis      1.0\n",
      "and           1.0\n",
      "applications  1.0\n",
      "artificial    1.0\n",
      "chatbots      1.0\n",
      "computers     1.0\n",
      "enables       1.0\n",
      "field         1.0\n",
      "generate      1.0\n",
      "human         1.0\n",
      "in            1.0\n",
      "including     1.0\n",
      "intelligence  1.0\n",
      "interpret     1.0\n",
      "is            1.0\n",
      "language      1.0\n",
      "natural       1.0\n",
      "nlp           1.0\n",
      "of            1.0\n",
      "processing    1.0\n",
      "sentiment     1.0\n",
      "that          1.0\n",
      "to            1.0\n",
      "translation   1.0\n",
      "understand    1.0\n",
      "used          1.0\n",
      "various       1.0\n"
     ]
    }
   ],
   "source": [
    "# Inverse Document Frequency (IDF)\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "idf_values = vectorizer.idf_  # Get the IDF values from the vectorizer\n",
    "idf_df = pd.DataFrame(idf_values, index=vectorizer.get_feature_names_out(), columns=[\"IDF\"])  # Create DataFrame\n",
    "print(idf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b712232-8399-481a-b5ac-09587a469cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "# Breaking text into smaller parts like words or sentences.\n",
    "# Example: \"I love NLP\" → [\"I\", \"love\", \"NLP\"]\n",
    "# POS (Part-of-Speech) Tagging\n",
    "# Labeling each word with its grammar role, like noun, verb, adjective.\n",
    "# Example: \"run\" → (\"run\", Verb) or (\"run\", Noun) depending on usage.\n",
    "# Stop Words Removal\n",
    "# Removing common words that don’t add much meaning.\n",
    "# Example: \"I am learning NLP\" → \"learning NLP\" (removed: \"I\", \"am\")\n",
    "#  Stemming\n",
    "# Reducing words to their root form by chopping suffixes.\n",
    "# Example: \"playing\", \"played\" → \"play\"\n",
    "# Lemmatization\n",
    "# Reducing words to their dictionary form (lemma) using grammar rules.\n",
    "# Example: \"better\" → \"good\" (correct base word)\n",
    "# TF (Term Frequency)\n",
    "# It shows how often a word appears in a document.\n",
    "# Formula: TF = (Number of times term appears in a document) / (Total number of terms in the document)\n",
    "# Example: \"I love NLP. NLP is great.\" → TF of \"NLP\" = 2 / 6 = 0.33\n",
    "# IDF (Inverse Document Frequency)\n",
    "# It shows how rare a word is across all documents.\n",
    "# Formula: IDF = log(Total number of documents / Number of documents containing the term)\n",
    "# Example: If \"NLP\" appears in 1 out of 10 documents → IDF = log(10 / 1) = 1\n",
    "# TF-IDF\n",
    "# It is the product of TF and IDF and highlights important words in a document.\n",
    "# Formula: TF-IDF = TF × IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79220d39-9f09-459e-ac6e-18a3e1cc5f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
